# ü§ó Configuraci√≥n de Hugging Face para Chat IA

## üìã Requisitos

1. Cuenta gratuita en Hugging Face
2. Token de acceso (Read)
3. Conexi√≥n a internet

## üöÄ Pasos de Configuraci√≥n

### 1. Crear cuenta en Hugging Face

Ve a [https://huggingface.co/join](https://huggingface.co/join) y crea una cuenta gratuita.

### 2. Generar Token de Acceso

1. Inicia sesi√≥n en tu cuenta
2. Ve a **Settings** ‚Üí **Access Tokens**: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
3. Haz clic en **"New token"**
4. Dale un nombre (ej: "CV-Chat-Token")
5. Selecciona el tipo: **Read**
6. Copia el token generado (algo como: `hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxx`)

### 3. Configurar en el proyecto

Abre el archivo `js/config.js` y pega tu token:

```javascript
const config = {
    hfToken: 'hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxx', // Tu token aqu√≠
    hfModel: 'mistralai/Mistral-7B-Instruct-v0.2',
    useHuggingFace: true, // Cambiar a true para activar
    useLocalFallback: true
};
```

## ü§ñ Modelos Recomendados

### 1. **Mistral-7B-Instruct-v0.2** (Recomendado)
```javascript
hfModel: 'mistralai/Mistral-7B-Instruct-v0.2'
```
- ‚úÖ R√°pido y preciso
- ‚úÖ Multiling√ºe (espa√±ol/ingl√©s)
- ‚úÖ Excelente para conversaciones profesionales
- ‚ö° Tiempo de respuesta: 2-4 segundos

### 2. **Llama-2-7B-Chat**
```javascript
hfModel: 'meta-llama/Llama-2-7b-chat-hf'
```
- ‚úÖ Muy conversacional
- ‚úÖ Respuestas naturales
- ‚ö†Ô∏è Puede requerir aprobaci√≥n de acceso

### 3. **Flan-T5-Base**
```javascript
hfModel: 'google/flan-t5-base'
```
- ‚úÖ M√°s r√°pido (1-2 segundos)
- ‚úÖ Gratuito sin l√≠mites
- ‚ö†Ô∏è Respuestas m√°s cortas

### 4. **Zephyr-7B-Beta**
```javascript
hfModel: 'HuggingFaceH4/zephyr-7b-beta'
```
- ‚úÖ Optimizado para instrucciones
- ‚úÖ Muy bueno en espa√±ol
- ‚ö° Velocidad media

## ‚öôÔ∏è Configuraci√≥n Avanzada

### Ajustar par√°metros de generaci√≥n

En `gemini-search.js`, funci√≥n `askHuggingFace()`:

```javascript
parameters: {
    max_new_tokens: 150,     // Longitud m√°xima de respuesta
    temperature: 0.7,        // Creatividad (0.1-1.0)
    top_p: 0.9,             // Diversidad de respuesta
    return_full_text: false
}
```

**Recomendaciones:**
- `temperature: 0.7` - Equilibrado (profesional pero natural)
- `temperature: 0.3` - M√°s conservador y preciso
- `temperature: 0.9` - M√°s creativo y variado

## üîÑ Sistema H√≠brido

El sistema funciona en **3 niveles**:

1. **Respuestas r√°pidas locales** (< 200ms)
   - Saludos, despedidas
   - Preguntas simples

2. **Hugging Face IA** (2-4 segundos)
   - Preguntas complejas
   - Conversaciones naturales
   - Contexto personalizado

3. **Fallback local** (300ms)
   - Si Hugging Face falla
   - Si no hay token configurado
   - Respuestas predefinidas inteligentes

## üìä Comparaci√≥n de Opciones

| Caracter√≠stica | Local | Hugging Face |
|----------------|-------|--------------|
| **Velocidad inicial** | 300ms ‚ö° | 2-4 seg |
| **Calidad respuesta** | Buena ‚≠ê‚≠ê‚≠ê | Excelente ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Conversacional** | Limitado | Natural üí¨ |
| **Costo** | Gratis üí∞ | Gratis üí∞ |
| **Configuraci√≥n** | No requiere | Token necesario |
| **Offline** | S√≠ üì¥ | No |

## üéØ Casos de Uso

### Usar Solo Local
```javascript
useHuggingFace: false
```
**Cu√°ndo:**
- Demo sin internet
- M√°xima velocidad
- No quieres configurar token

### Usar Hugging Face
```javascript
useHuggingFace: true
```
**Cu√°ndo:**
- Conversaciones m√°s naturales
- Respuestas personalizadas
- Mejor comprensi√≥n de contexto

## üêõ Soluci√≥n de Problemas

### Error: "Invalid token"
- Verifica que copiaste el token completo
- Aseg√∫rate de que empiece con `hf_`
- Regenera el token si es necesario

### Error: "Model is loading"
- Es normal la primera vez
- Espera 20-30 segundos y reintenta
- El modelo se "despierta" despu√©s de inactividad

### Respuestas lentas
- Cambia a un modelo m√°s peque√±o (flan-t5-base)
- Reduce `max_new_tokens` a 100
- El primer request siempre es m√°s lento

### Respuestas en ingl√©s
- Aseg√∫rate de que el prompt incluya "Responde en espa√±ol"
- Usa modelos multiling√ºes como Mistral
- Ajusta la temperatura a 0.6-0.7

## üí° Tips de Optimizaci√≥n

1. **Primera pregunta siempre m√°s lenta** - El modelo se carga en memoria
2. **Preguntas siguientes son m√°s r√°pidas** - Modelo ya cargado
3. **Sistema h√≠brido inteligente** - Respuestas r√°pidas van local, complejas van a HF
4. **Fallback autom√°tico** - Si HF falla, usa sistema local
5. **Badge indicador** - Muestra qu√© sistema est√° activo

## üìù Ejemplo de Uso

```javascript
// En config.js
const config = {
    hfToken: 'hf_TuTokenAqui123456789',
    hfModel: 'mistralai/Mistral-7B-Instruct-v0.2',
    useHuggingFace: true,
    useLocalFallback: true
};
```

**Resultado:**
- Preguntas simples: Respuesta local inmediata ‚ö°
- Preguntas complejas: IA de Hugging Face ü§ñ
- Error o sin token: Fallback local üîÑ

---

**üéâ ¬°Listo para usar! El chat ahora tiene IA conversacional de √∫ltima generaci√≥n.**

## üîó Enlaces √ötiles

- [Hugging Face Models](https://huggingface.co/models)
- [Inference API Docs](https://huggingface.co/docs/api-inference/index)
- [Token Settings](https://huggingface.co/settings/tokens)
- [Model Pricing](https://huggingface.co/pricing) (Inference API es gratis)
